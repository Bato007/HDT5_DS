{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import string, re, emoji\n",
    "\n",
    "# Limpieza de textos\n",
    "from pattern.text.en import singularize, lemma\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from transformers import BertTokenizer,TFBertModel\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy,BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy,BinaryAccuracy\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch\n",
    "\n",
    "\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "  id - a unique identifier for each tweet\n",
    "  text - the text of the tweet\n",
    "  location - the location the tweet was sent from (may be blank)\n",
    "  keyword - a particular keyword from the tweet (may be blank)\n",
    "  target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\n",
    "'''\n",
    "train = pd.read_csv('./train.csv', encoding='utf8')\n",
    "test = pd.read_csv('./test.csv', encoding='utf8')\n",
    "\n",
    "cachedStopWords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis Exploratorio I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['location'].value_counts().head(10).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['keyword'].value_counts().head(10).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['target'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanCountry(value):\n",
    "  try:\n",
    "\n",
    "    if (\n",
    "      'nan' in value or\n",
    "      'world' in value or\n",
    "      'global' in value or\n",
    "      'everywhere' in value or\n",
    "      'earth' in value or\n",
    "      'ss' in value or\n",
    "      '?' in value or\n",
    "      re.search(r\"[0-9]+\", value, re.I)\n",
    "    ):\n",
    "      return 'unknow'\n",
    "\n",
    "    if (\n",
    "      'italy' in value\n",
    "    ):\n",
    "      return 'italy'\n",
    "\n",
    "    if (\n",
    "      'india' in value or\n",
    "      'mumbai' in value\n",
    "    ):\n",
    "      return 'india'\n",
    "    \n",
    "    if (\n",
    "      'switzerland' in value or\n",
    "      'geneva' in value\n",
    "    ):\n",
    "      return 'switzerland'\n",
    "    \n",
    "    if (\n",
    "      'china' in value or\n",
    "      'hong kong' in value\n",
    "    ):\n",
    "      return 'china'\n",
    "\n",
    "    if (\n",
    "      'nigeria' in value or\n",
    "      'lagos' in value\n",
    "    ):\n",
    "      return 'nigeria'\n",
    "\n",
    "    if (\n",
    "      'japan' in value or\n",
    "      'tokyo' in value\n",
    "    ):\n",
    "      return 'japan'\n",
    "\n",
    "    if (\n",
    "      'ontario' in value or\n",
    "      'canada' in value or\n",
    "      'toronto' in value or\n",
    "      'calgary' in value or\n",
    "      'alberta' in value or\n",
    "      re.search(r\"ab$\", value, re.I) or\n",
    "      re.search(r\"bc$\", value, re.I)\n",
    "    ):\n",
    "      return 'canada'\n",
    "\n",
    "    if (\n",
    "      'uk' == value or\n",
    "      'united kingdom' in value or\n",
    "      'kingdom' in value or\n",
    "      'british' in value or\n",
    "      'scotland' in value or\n",
    "      'newcastle' in value or\n",
    "      'england' in value or\n",
    "      'london' in value or\n",
    "      re.search(r\"uk$\", value, re.I)\n",
    "    ):\n",
    "      return 'uk'\n",
    "\n",
    "    if (\n",
    "      'nyc' == value or\n",
    "      'nj' == value or\n",
    "      'united states' in value or\n",
    "      'new york' in value or\n",
    "      'san francisco' in value or\n",
    "      'los angeles' in value or\n",
    "      'new jersey' in value or\n",
    "      'north carolina' in value or\n",
    "      'st. louis' in value or\n",
    "      'kansas city' in value or\n",
    "      'san diego' in value or\n",
    "      'las vegas' in value or\n",
    "      'sacramento' in value or\n",
    "      'oregon' in value or\n",
    "      'michigan' in value or\n",
    "      'manchester' in value or\n",
    "      'portland' in value or\n",
    "      'texas' in value or\n",
    "      'u.s.' in value or\n",
    "      'philippines' in value or\n",
    "      'nevada' in value or\n",
    "      'us' in value or\n",
    "      'arizona' in value or\n",
    "      'lincoln' in value or\n",
    "      'wisconsin' in value or\n",
    "      'pennsylvania' in value or\n",
    "      'seattle' in value or\n",
    "      'usa' in value or\n",
    "      'washington' in value or\n",
    "      'florida' in value or\n",
    "      'chicago' in value or\n",
    "      'california' in value or\n",
    "      'nashville' in value or\n",
    "      'colorado' in value or\n",
    "      'denver' in value or\n",
    "      'cleveland' in value or\n",
    "      'atlanta' in value or\n",
    "      'massachusetts' in value or\n",
    "      'boston' in value or\n",
    "      'oklahoma' in value or\n",
    "      'tennessee' in value or\n",
    "      'liverpool' in value or\n",
    "      'phoenix' in value or\n",
    "      'baltimore' in value or\n",
    "      re.search(r\"nyc$\", value, re.I) or\n",
    "      re.search(r\"hi$\", value, re.I) or\n",
    "      re.search(r\"va$\", value, re.I) or\n",
    "      re.search(r\"ks$\", value, re.I) or\n",
    "      re.search(r\"la$\", value, re.I) or\n",
    "      re.search(r\"ak$\", value, re.I) or\n",
    "      re.search(r\"md$\", value, re.I) or\n",
    "      re.search(r\"mo$\", value, re.I) or\n",
    "      re.search(r\"wi$\", value, re.I) or\n",
    "      re.search(r\"az$\", value, re.I) or\n",
    "      re.search(r\"ga$\", value, re.I) or\n",
    "      re.search(r\"ok$\", value, re.I) or\n",
    "      re.search(r\"nj$\", value, re.I) or\n",
    "      re.search(r\"wa$\", value, re.I) or\n",
    "      re.search(r\"pa$\", value, re.I) or\n",
    "      re.search(r\"ma$\", value, re.I) or\n",
    "      re.search(r\"co$\", value, re.I) or\n",
    "      re.search(r\"oh$\", value, re.I) or\n",
    "      re.search(r\"il$\", value, re.I) or\n",
    "      re.search(r\"tn$\", value, re.I) or\n",
    "      re.search(r\"dc$\", value, re.I) or\n",
    "      re.search(r\"ca$\", value, re.I) or\n",
    "      re.search(r\"tx$\", value, re.I) or\n",
    "      re.search(r\"nc$\", value, re.I) or\n",
    "      re.search(r\"fl$\", value, re.I) or\n",
    "      re.search(r\"ny$\", value, re.I)\n",
    "    ):\n",
    "      return 'usa'\n",
    "\n",
    "    return value\n",
    "  except:\n",
    "    return 'unknow'\n",
    "\n",
    "train['location'] = train['location'].apply(lambda row: str(row).lower())\n",
    "train['location'] = train['location'].apply(lambda row: cleanCountry(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toSingular(value):\n",
    "  try:\n",
    "    return singularize(value)\n",
    "  except:\n",
    "    return value\n",
    "\n",
    "def parseLemma(value):\n",
    "  try:\n",
    "    return lemma(value)\n",
    "  except:\n",
    "    return value\n",
    "\n",
    "def replaceSpace(value):\n",
    "  return str(value).replace('%20', ' ')\n",
    "\n",
    "train['keyword'] = train['keyword'].apply(lambda row: toSingular(row))\n",
    "train['keyword'] = train['keyword'].apply(lambda row: parseLemma(row))\n",
    "train['keyword'] = train['keyword'].apply(lambda row: replaceSpace(row))\n",
    "train['keyword'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeHastags(value):\n",
    "  if ('#' not in value): return value\n",
    "  hashtags = re.findall(r\"#[^\\s]*\", value, re.I)\n",
    "  for hashtag in hashtags:\n",
    "    value = value.replace(hashtag, '')\n",
    "  return value\n",
    "\n",
    "def removeLinks(value):\n",
    "  if ('http' not in value): return value\n",
    "  links = re.findall(r\"http[^\\s]*\", value, re.I)\n",
    "  for link in links:\n",
    "    value = value.replace(link, '')\n",
    "  return value\n",
    "\n",
    "def removeStepWords(value):\n",
    "  return ' '.join([word for word in value.split() if word not in cachedStopWords])\n",
    "\n",
    "def extractEmojis(value):\n",
    "  items = value.split(' ')\n",
    "  emojis = ''.join(item for item in items if item in emoji.EMOJI_DATA)\n",
    "\n",
    "  if (len(emojis) > 0): print(value)\n",
    "\n",
    "  return value\n",
    "\n",
    "def sentenceToSingular(value):\n",
    "  items = value.split(' ')\n",
    "  for item in items:\n",
    "    singular = toSingular(item)\n",
    "    value = value.replace(item, singular)\n",
    "  return value\n",
    "\n",
    "def sentenceToPresent(value):\n",
    "  items = value.split(' ')\n",
    "  for item in items:\n",
    "    present = parseLemma(item)\n",
    "    value = value.replace(item, present)\n",
    "  return value\n",
    "\n",
    "translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "\n",
    "cleanText = []\n",
    "for index, row in train.iterrows():\n",
    "  text = row['text'][:]\n",
    "\n",
    "  # Limpiando el texto\n",
    "  text = text.lower()                         # Convierte todo a minusculas\n",
    "  text = text.replace('utc', '')              # Quita utc\n",
    "  text = text.replace('#', '')                # Quita #\n",
    "  text = text.replace('@', '')                # Quita @\n",
    "  text = removeLinks(text)                    # Quita links\n",
    "  text = extractEmojis(text)                  # Quita todos los emojis\n",
    "  text = text.translate(translator)           # Quita todos los signos de puntuacion\n",
    "  text = removeStepWords(text)                # Quita todas las step words\n",
    "  text = re.sub('  +', ' ', text)             # Quita todos los espacios de mas\n",
    "  text = sentenceToSingular(text)             # Pasa las palabras a singular\n",
    "  text = sentenceToPresent(text)              # Pasa las palabras a presente\n",
    "  \n",
    "  numbers = re.findall(r\"[0-9]\", text, re.I)\n",
    "  if (len(numbers) > 0):\n",
    "    for number in numbers:\n",
    "      if (number == '911'): continue\n",
    "\n",
    "      # Quitando numeros\n",
    "      text = text.replace(number, '')\n",
    "\n",
    "  text = removeStepWords(text)                # Quita todas las step words\n",
    "  text = re.sub('  +', ' ', text)             # Quita todos los espacios de mas\n",
    "\n",
    "  cleanText.append(text)\n",
    "\n",
    "train['text'] = cleanText[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in train.iterrows():\n",
    "  print(row['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis Exploratorio II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['keyword'].value_counts().head(10).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['location'].value_counts().head(10).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train['target'] == 0]['keyword'].value_counts().head(15).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train['target'] == 1]['keyword'].value_counts().head(15).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disaster Tweets wordcloud \n",
    "disaster_tweets = train[train['target'] == 1]\n",
    "disaster_string = []\n",
    "for t in disaster_tweets.text:\n",
    "    disaster_string.append(t)\n",
    "disaster_string = pd.Series(disaster_string).str.cat(sep=' ')\n",
    "wordcloud = WordCloud(width=1600, height=800,max_font_size=100, background_color='white').generate(disaster_string)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive tweets wordcloud\n",
    "formal_tweets = train[train[\"target\"] == 0]\n",
    "formal_string = []\n",
    "for t in formal_tweets.text:\n",
    "    formal_string.append(t)\n",
    "formal_string = pd.Series(formal_string).str.cat(sep=' ')\n",
    "wordcloud = WordCloud(width=1600, height=800,max_font_size=100, background_color='white').generate(formal_string)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificando tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity = []\n",
    "subjectivity = []\n",
    "positive = []\n",
    "\n",
    "for text in train.text:\n",
    "    sent = TextBlob(text)\n",
    "    polarity = sent.sentiment.polarity\n",
    "    subjectivity  = sent.sentiment.subjectivity\n",
    "\n",
    "    sent = TextBlob(text, analyzer = NaiveBayesAnalyzer())\n",
    "    classification= sent.sentiment.classification\n",
    "    positiveScore = sent.sentiment.p_pos\n",
    "\n",
    "    polarity.append(polarity)\n",
    "    subjectivity.append(subjectivity)\n",
    "    positive.append(positiveScore)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separando train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train.text.values, train.target.values, test_size=0.1, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_for_bert(data):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sentence in data:\n",
    "        encoded_sentence = tokenizer.encode_plus(\n",
    "            text=sentence,\n",
    "            add_special_tokens=True,       \n",
    "            max_length=MAX_LEN,                  \n",
    "            pad_to_max_length=True,         \n",
    "            return_attention_mask=True     \n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sentence.get('input_ids'))\n",
    "        attention_masks.append(encoded_sentence.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenando y codificando train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = np.concatenate([train.text.values, test.text.values])\n",
    "\n",
    "# Encode our concatenated data\n",
    "encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in all_tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocessing_for_bert en X_train y X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 64\n",
    "\n",
    "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
    "val_inputs, val_masks = preprocessing_for_bert(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construyendo el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.tensor(y_train)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
    "\n",
    "test_data = TensorDataset(val_inputs, val_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(torch.nn.Module):\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(768, 50),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(50, 2)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(epochs=4):\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=5e-5,\n",
    "                      eps=1e-8 \n",
    "                      )\n",
    "\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision y perdida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediccion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprobando las predicciones"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8fa13260b2ad5da8a92d4362f230c03eee7b3dfd5b47a73cfd4853277a201fcb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
